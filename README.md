# Cobra Video Analysis Tool

This tool analyzes videos using Large Language Models (like GPT-4 Vision) to provide structured descriptions of content, including chapters, persons, actions, and objects, with a focus on generating accurate timestamps.

## Features

-   **Flexible Segmentation:**
    *   Segments video based on content using **PySceneDetect** (if `--use-scene-detection` is enabled, highest priority).
    *   Falls back to segmentation based on **natural speech pauses** found in the transcription (if `--use-speech-based-segments` is enabled, audio is present, and scene detection is disabled).
    *   Uses **fixed-time intervals** (`--segment-duration`) as the final fallback method.
-   **Dual Analysis Strategy:**
    *   **Chapters:** Generated by analyzing the full context (frames, transcription) of each video segment for narrative understanding.
    *   **Tags (Persons, Actions, Objects):** Generated by analyzing segments in smaller, consecutive time chunks (default: 5 seconds, configurable in code) using frames and their precise timestamps to achieve higher temporal accuracy for when entities appear or actions occur.
-   **Timestamp Accuracy:** Aims for second-level accuracy in global tags by providing the LLM with frames timestamped precisely during preprocessing (typically 1 frame per second, configurable via `--fps`).
-   **Frame Handling:** Currently processes all extracted frames within each segment/chunk. Future optimizations may involve intelligent frame selection (e.g., keyframes), but the current method maximizes the information available to the LLM for tagging.
-   **Transcription Integration:** Leverages **Azure Batch Transcription** (if audio is available and configured). This provides:
    *   A full transcript string within the `_ActionSummary.json`.
    *   Detailed `transcriptionDetails` in `_ActionSummary.json`, including per-segment information (speaker, timings, text variants).
    *   Richer context for both chapter and tag generation.
    *   Requires Azure Speech and Blob Storage setup.
-   **Dominant Color Extraction:** Extracts dominant colors from video frames during preprocessing, providing these as hex values for scene/segment characterization in the output.
-   **Customizable Entity Recognition:** Allows providing lists of known persons, actions, objects, emotions, and themes to guide the analysis.
-   **Tag Post-Processing:** Includes logic for normalizing tag names (e.g., "Walking" vs "walking") and merging adjacent or overlapping time intervals for the same tag based on a configurable time gap during the final aggregation step.
-   **Asynchronous Processing:** Supports parallel processing of segments and tagging chunks for faster analysis on multi-core systems (`--async`).
-   **Structured JSON Output:** Produces detailed JSON files containing the analysis results, including a final aggregated summary file.

## Installation

1.  Ensure you have Python 3.8+ and `ffmpeg` installed and available in your system's PATH.
2.  Clone the repository.
3.  Install the required Python packages:
    ```bash
    pip install -r requirements.txt
    ```
4.  **Optional:** If using scene-based segmentation (`--use-scene-detection`), ensure you have `scenedetect` installed, potentially with OpenCV support for better performance:
    ```bash
    # Choose one depending on your OpenCV setup preference
    pip install scenedetect[opencv]
    # or
    pip install scenedetect[opencv-headless]
    ```
5.  Set up your API credentials (e.g., Azure OpenAI, Azure Speech, Azure Storage) either via environment variables or a `.env` file (see Configuration).

## Configuration

API credentials and endpoints are required. You can configure them using:

1.  **Environment Variables:**
    *   `AZURE_OPENAI_GPT_VISION_ENDPOINT` (Required for analysis)
    *   `AZURE_OPENAI_GPT_VISION_API_KEY` (Required for analysis)
    *   `AZURE_OPENAI_GPT_VISION_API_VERSION` (Required for analysis)
    *   `AZURE_OPENAI_GPT_VISION_DEPLOYMENT` (Required for analysis)
    *   `AZURE_SPEECH_KEY` (Required for transcription)
    *   `AZURE_SPEECH_REGION` (Required for transcription)
    *   `AZURE_STORAGE_ACCOUNT_NAME` (Required for transcription - audio upload)
    *   `AZURE_STORAGE_CONTAINER_NAME` (Required for transcription - audio upload)
    *   `AZURE_STORAGE_CONNECTION_STRING` (Required for transcription - OR use SAS_TOKEN)
    *   `AZURE_STORAGE_SAS_TOKEN` (Required for transcription - OR use CONNECTION_STRING)
    *   (Optional) `AZURE_FACE_ENDPOINT`, `AZURE_FACE_API_KEY` (for face recognition - *Currently experimental*)
2.  **`.env` File:** Create a `.env` file in the project root or specify a path using `--env-file`.
3.  **Command Line Arguments:** Override environment settings using arguments like `--api-key`, `--api-base`, etc.

## Usage

Analysis using scene detection (recommended for content-aware segments):

```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --use-scene-detection --scene-detection-threshold 27.0
```

Analysis using time-based segments:

```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --segment-duration 60
```

Analysis using speech-based segmentation (requires audio and transcription configuration):

```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --use-speech-based-segments
```

Analysis with custom lists and asynchronous processing:

```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --fps 1.0 --async --actions-list path/to/actions.json --objects-list path/to/objects.json --use-scene-detection
```

### Command Line Arguments

*   `video_path`: Path to the video file (required).
*   `--output-dir`: Directory to save analysis results (default: `./{video_name}_analysis`).
*   `--env-file`: Path to a custom `.env` file.
*   `--api-key`: Vision API key (overrides environment).
*   `--api-base`: Vision API endpoint (overrides environment).
*   `--api-version`: Vision API version (overrides environment).
*   `--deployment-name`: Vision API deployment name (overrides environment).
*   `--fps`: Frames per second to extract during preprocessing (default: 1.0). Affects granularity available for tagging.
*   `--use-scene-detection`: Use PySceneDetect to determine segments instead of fixed intervals or speech pauses (Highest priority).
*   `--scene-detection-threshold`: PySceneDetect `ContentDetector` threshold (default: 30.0). Lower value generally means more, shorter scenes. Used only if `--use-scene-detection` is active.
*   `--use-speech-based-segments`: Use speech analysis to define segment boundaries. Ignored if `--use-scene-detection` is active. Requires audio and successful transcription.
*   `--segment-duration`: Default length (seconds) for time-based video segments (default: 30.0). Used only if scene detection and speech-based segmentation are inactive or fail.
*   `--transcription-path`: Path to an existing JSON transcription file (if not generating via Azure).
*   `--async`: Run segment/chunk analysis asynchronously using multiple processes/threads (default: False).
*   `--peoples-list`: Path to JSON file defining known people.
*   `--emotions-list`: Path to JSON file defining relevant emotions.
*   `--objects-list`: Path to JSON file defining known objects.
*   `--themes-list`: Path to JSON file defining potential themes.
*   `--actions-list`: Path to JSON file defining known actions.
*   `--copyright-file`: Path to a JSON file containing copyright information to be included in the `ActionSummary` output.
*   `--downscale-to-max-width`: Maximum width for downscaled frames (maintains aspect ratio).
*   `--downscale-to-max-height`: Maximum height for downscaled frames (maintains aspect ratio).

### Custom List Format

Custom lists (e.g., for persons, objects, actions) allow providing domain-specific guidance to the LLM. The tool expects a JSON file containing:
1.  A top-level key matching the list type (e.g., `"actions"`, `"persons"`, `"objects"`). The value should be an array of objects.
2.  *(Optional but Recommended)* A top-level key named `"instructions"` containing a string of text. **This `instructions` text is the primary mechanism currently used to customize the LLM's behavior for tag generation.** It is directly injected into the system prompt for identifying persons, actions, and objects.
3.  Each object within the array (e.g., under `"actions"`) typically contains:
    *   A key for the item's name (e.g., `"name"` or `"label"` - the exact key name used here is **not** currently critical for prompt generation, although `"name"` aligns with the final output structure).
    *   A `"description"` key.

**Important:** While the structure including item names (`name`/`label`) and `description` is loaded, the current implementation for tag generation **only explicitly uses the top-level `"instructions"` string** in the prompt. The individual item `name`/`label` and `description` fields are **not** currently formatted into the tag generation prompts. They serve primarily as user reference or may provide *implicit* context if the `label` names are common terms the LLM recognizes.

Example (`actions.json`):

```json
{
  "actions": [ // Top-level key matching list type
    {
      "name": "Typing", // Item name (key doesn't strictly matter for prompt, 'name' matches output)
      "description": "Person using a keyboard." // Description (currently not used in prompt)
    },
    {
      "name": "Presenting",
      "description": "Person speaking to an audience, possibly using slides or gestures."
    }
  ],
  // This is the key currently used for explicit prompt guidance for tags:
  "instructions": "Focus on identifying professional actions in an office setting."
}
```

## Output Structure

The analysis creates a structured output directory:

*   `_ActionSummary.json`: (Or named based on the analysis config, e.g., `_BasicSummary.json`). This is the **final, aggregated output file**. It contains processed chapters, global tags (persons, actions, objects) with merged time intervals, metadata, and copyright info (if provided). If transcription was performed, it also includes:
    *   A `transcript` field with the full concatenated text.
    *   A `transcriptionDetails` object containing structured segments with speaker labels, start/end times, confidence scores, and text variants.
    This file is generated by the `process_segment_results` method of the chosen analysis configuration.
*   `_video_manifest.json`: Contains all processing metadata, source video info, segment definitions, and the *raw, unaggregated* analysis results returned by the LLM for each segment/chunk *before* the final aggregation and merging step. If Azure Batch Transcription is used, the full raw JSON output from the Azure service is typically stored here before being processed and integrated into the `_ActionSummary.json`. **Note:** After final aggregation into `_ActionSummary.json`, the `global_tags` within individual segment results in *this* file are typically removed for clarity, as the merged version exists in the final summary file.
*   Segment Folders (`seg_...` or `scene_...`): Each segment has its own folder containing:
    *   `frames/`: Extracted image frames (named with their absolute timestamps, e.g., `frame_0_12.345s.jpg`).
    *   `_segment_analyzed_result_ActionSummary.json`: The raw JSON output from the LLM for that segment (including both chapter and chunked tag results before final aggregation).
    *   `_segment_prompt.json`: The prompt sent to the LLM for this segment/chunk.

## How it Works: Chapters vs. Tags

The tool employs two distinct strategies for analysis:

1.  **Chapter Generation:** To understand the narrative and overall content of a segment (defined by scene, speech, or time), the LLM analyzes *all* frames and the full transcription (if available and applicable for the segment) for that segment in a single call. This provides broad context for generating a descriptive chapter summary, theme, etc.
2.  **Tag Generation (Persons, Actions, Objects):** To achieve accurate start and end times for specific tags, the analysis is done differently. Each segment is broken down into smaller, consecutive time *chunks* (default: 5 seconds, configurable in `video_analyzer.py`). The LLM analyzes the frames *within each chunk*, along with any corresponding transcription text for that time window (if available), using their precise timestamps. This forces the LLM to focus on a shorter time window, significantly improving the accuracy of the `start` and `end` timecodes associated with each detected person, action, or object. The results from all chunks across all segments are then merged and post-processed (time intervals combined based on proximity) by the `process_segment_results` method (e.g., in `ActionSummary`) to create the final `global_tags` in the main output file (`_ActionSummary.json`).

This dual approach balances the need for broad segment understanding (for chapters) with the need for temporal precision (for tags). The flexible segmentation allows the segments themselves to be more contextually relevant.

## Note on Frame Storage
In production environments, consider strategies for managing extracted frames (e.g., storing them externally, deleting after processing) as they can consume significant disk space. Currently, all extracted frames are saved and not deleted when the process finishes.

## Operational Notes

*   **Azure Blob Storage Cleanup:** When using Azure Batch Transcription, you might observe `BlobNotFound` errors in the logs during the cleanup phase for temporary audio files uploaded to Azure Blob Storage. This is often normal. The Azure Speech service itself may delete the input audio blob from the container after it has successfully processed the transcription. The tool attempts to delete these blobs as a precautionary cleanup measure, so these errors usually indicate the file was already cleaned up by Azure.