# Video Scribe Documentation

This tool analyzes videos using Large Language Models (like GPT 4.1) to provide structured descriptions of content, including chapters, persons, actions, and objects, with a focus on generating accurate timestamps and leveraging advanced object tracking.

## Features

-   **Flexible Segmentation:**
    *   Segments video based on content using **PySceneDetect** (if `--use-scene-detection` is enabled, highest priority).
    *   Falls back to segmentation based on **natural speech pauses** found in the transcription (if `--use-speech-based-segments` is enabled, audio is present, and scene detection is disabled).
    *   Uses **fixed-time intervals** (`--segment-duration`) as the final fallback method.
-   **Advanced Object/Person Tracking with YOLO:**
    *   **Initial Raw Detection:** Employs YOLO (e.g., `yolo11x.pt` with ByteTrack via `step0_yolo_track.py`) for initial detection and tracking of objects and persons within the video, generating raw track data with frame-level bounding boxes and preliminary IDs.
    *   **Cross-Scene Track Refinement:** An optional but recommended step (`track_refiner.py`) uses an LLM to analyze the raw YOLO tracks across detected scene boundaries. This process aims to:
        *   Re-identify and link tracks of the same person/object that might have been broken by scene changes or temporary occlusions.
        *   Assign more stable and persistent `refined_track_id`s.
        *   Generate descriptive labels (e.g., "Man in blue shirt") for these refined tracks using visual information from representative frames.
    *   **Contextual Enhancement:** The refined (or raw, if refinement is skipped or fails) YOLO tracks, along with their generated descriptions and IDs, are provided as context to the main LLM analysis for each segment. This helps the LLM to be aware of known entities.
-   **Dual Analysis Strategy:**
    *   **Chapters:** Generated by analyzing the full context (frames, transcription, relevant YOLO tracks) of each video segment for narrative understanding.
    *   **Tags (Persons, Actions, Objects):**
        *   **Persons:** Leverages the (refined) YOLO track IDs for consistent identification. Descriptions are often derived from the track refinement step or further LLM analysis of track appearances. The final aggregation in `ActionSummary` groups persons by their stable `id`, merges their timecodes, and selects a representative name.
        *   **Objects:** Similar to persons, object tracks from YOLO are used. If not fully described during refinement, `VideoAnalyzer` might use an LLM to generate a descriptive label for a detected object based on its visual crop. These are then aggregated by name.
        *   **Actions:** Identified through LLM analysis of segment frames and transcription, potentially informed by the presence of tracked persons/objects. A dedicated fine-grained action extraction step can also run per segment.
-   **Timestamp Accuracy:** Aims for second-level accuracy in global tags by providing the LLM with frames timestamped precisely during preprocessing (typically 1 frame per second, configurable via `--fps`). YOLO tracking also provides frame-level timestamps.
-   **Frame Handling:** Currently processes all extracted frames within each segment/chunk for LLM analysis. YOLO processes the video at its native frame rate.
-   **Transcription Integration:** Leverages **Azure Batch Transcription** (if audio is available and configured). This provides:
    *   A full transcript string within the `_ActionSummary.json`.
    *   Detailed `transcriptionDetails` in `_ActionSummary.json`, including per-segment information (speaker, timings, text variants).
    *   Richer context for both chapter and tag generation.
    *   Requires Azure Speech and Blob Storage setup.
-   **Dominant Color Extraction:** Extracts dominant colors from video frames during preprocessing, providing these as hex values for scene/segment characterization in the output.
-   **Customizable Entity Recognition:** Allows providing lists of known persons, actions, objects, emotions, and themes to guide the analysis. The primary mechanism for this is the "instructions" field in these JSON lists.
-   **Tag Post-Processing:** The `ActionSummary` configuration includes logic for normalizing tag names and merging adjacent or overlapping time intervals for the same tag (or same person ID) based on a configurable time gap during the final aggregation step.
-   **Asynchronous Processing:** Supports parallel processing of segments for faster analysis on multi-core systems (`--run-async-analyzer`). The track refinement step also utilizes asynchronous LLM calls.
-   **Structured JSON Output:** Produces detailed JSON files containing the analysis results, including a final aggregated summary file.

## Installation

1.  Ensure you have Python 3.8+ and `ffmpeg` installed and available in your system's PATH.
2.  Clone the repository.
3.  Install the required Python packages:
    ```bash
    pip install -r requirements.txt 
    ```
    This includes `ultralytics` for YOLO, `opencv-python` for video processing, and `scenedetect` among others.
4.  **YOLO Models:** The `step0_yolo_track.py` script expects YOLO model files (e.g., `yolo11x.pt` for object detection) and tracker configuration files (e.g., `bytetrack.yaml`). These files should typically be placed in the `src/cobrapy/pipeline/` directory or be accessible from there. If they are not found, the YOLO tracking step will fail. *(Note: Specific model versions and paths might be hardcoded or configurable via script arguments in the future; check `run_yolo` function in `step0_yolo_track.py` for current expectations).*
5.  Set up your API credentials (e.g., Azure OpenAI, Azure Speech, Azure Storage) either via environment variables or a `.env` file (see Configuration).

## Configuration

API credentials and endpoints are required. You can configure them using:

1.  **Environment Variables:**
    *   `AZURE_OPENAI_GPT_VISION_ENDPOINT` (Required for analysis)
    *   `AZURE_OPENAI_GPT_VISION_API_KEY` (Required for analysis)
    *   `AZURE_OPENAI_GPT_VISION_API_VERSION` (Required for analysis)
    *   `AZURE_OPENAI_GPT_VISION_DEPLOYMENT` (Required for analysis)
    *   `AZURE_SPEECH_KEY` (Required for transcription)
    *   `AZURE_SPEECH_REGION` (Required for transcription)
    *   `AZURE_STORAGE_ACCOUNT_NAME` (Required for transcription - audio upload)
    *   `AZURE_STORAGE_CONTAINER_NAME` (Required for transcription - audio upload)
    *   `AZURE_STORAGE_CONNECTION_STRING` (Required for transcription - OR use SAS_TOKEN)
    *   `AZURE_STORAGE_SAS_TOKEN` (Required for transcription - OR use CONNECTION_STRING)
    *   (Optional) `AZURE_FACE_ENDPOINT`, `AZURE_FACE_API_KEY` (for face recognition - *Currently experimental*)
2.  **`.env` File:** Create a `.env` file in the project root or specify a path using `--env-file`.
3.  **Command Line Arguments:** Override environment settings using arguments like `--api-key`, `--api-base`, etc.

## Usage

Analysis using scene detection (recommended for content-aware segments), including YOLO refinement:

```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --use-scene-detection --scene-detection-threshold 27.0
```

To skip the YOLO track refinement step (e.g., for faster processing or if raw tracks are sufficient):
```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --use-scene-detection --skip-refinement
```

Analysis using time-based segments:

```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --segment-duration 60
```

Analysis using speech-based segmentation (requires audio and transcription configuration):

```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --use-speech-based-segments
```

Analysis with custom lists and asynchronous segment processing:

```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --fps 1.0 --run-async-analyzer --actions-list path/to/actions.json --objects-list path/to/objects.json --use-scene-detection
```

### Command Line Arguments

*   `video_path`: Path to the video file (required).
*   `--output-dir`: Directory to save analysis results (default: `./{video_name}_analysis`).
*   `--env-file`: Path to a custom `.env` file.
*   `--api-key`: Vision API key (overrides environment).
*   `--api-base`: Vision API endpoint (overrides environment).
*   `--api-version`: Vision API version (overrides environment).
*   `--deployment-name`: Vision API deployment name (overrides environment).
*   `--fps`: Frames per second to extract during preprocessing for LLM analysis (default: 1.0). Affects granularity available for LLM-based tagging. YOLO may process at the video's native FPS.
*   `--use-scene-detection`: Use PySceneDetect to determine segments instead of fixed intervals or speech pauses (Highest priority).
*   `--scene-detection-threshold`: PySceneDetect `ContentDetector` threshold (default: 30.0). Lower value generally means more, shorter scenes. Used only if `--use-scene-detection` is active.
*   `--use-speech-based-segments`: Use speech analysis to define segment boundaries. Ignored if `--use-scene-detection` is active. Requires audio and successful transcription.
*   `--segment-duration`: Default length (seconds) for time-based video segments (default: 30.0). Used only if scene detection and speech-based segmentation are inactive or fail.
*   `--transcription-path`: Path to an existing JSON transcription file (if not generating via Azure).
*   `--run-async-analyzer`: Run VideoAnalyzer's segment processing asynchronously (default: False).
*   `--skip-refinement`: Skip the YOLO track refinement step (default: False). If skipped, raw YOLO tracks are used directly.
*   `--peoples-list`: Path to JSON file defining known people.
*   `--emotions-list`: Path to JSON file defining relevant emotions.
*   `--objects-list`: Path to JSON file defining known objects.
*   `--themes-list`: Path to JSON file defining potential themes.
*   `--actions-list`: Path to JSON file defining known actions.
*   `--copyright-file`: Path to a JSON file containing copyright information to be included in the `ActionSummary` output.
*   `--downscale-to-max-width`: Maximum width for downscaled frames (maintains aspect ratio).
*   `--downscale-to-max-height`: Maximum height for downscaled frames (maintains aspect ratio).

### Custom List Format

Custom lists (e.g., for persons, objects, actions) allow providing domain-specific guidance to the LLM. The tool expects a JSON file containing:
1.  A top-level key matching the list type (e.g., `"actions"`, `"persons"`, `"objects"`). The value should be an array of objects.
2.  *(Optional but Recommended)* A top-level key named `"instructions"` containing a string of text. **This `instructions` text is the primary mechanism currently used to customize the LLM's behavior for tag generation.** It is directly injected into the system prompt for identifying persons, actions, and objects.
3.  Each object within the array (e.g., under `"actions"`) typically contains:
    *   A key for the item's name (e.g., `"name"` or `"label"` - the exact key name used here is **not** currently critical for prompt generation, although `"name"` aligns with the final output structure).
    *   A `"description"` key.

**Important:** While the structure including item names (`name`/`label`) and `description` is loaded, the current implementation for tag generation **only explicitly uses the top-level `"instructions"` string** in the prompt. The individual item `name`/`label` and `description` fields are **not** currently formatted into the tag generation prompts. They serve primarily as user reference or may provide *implicit* context if the `label` names are common terms the LLM recognizes.

Example (`actions.json`):

```json
{
  "actions": [ // Top-level key matching list type
    {
      "name": "Typing", // Item name (key doesn't strictly matter for prompt, 'name' matches output)
      "description": "Person using a keyboard." // Description (currently not used in prompt)
    },
    {
      "name": "Presenting",
      "description": "Person speaking to an audience, possibly using slides or gestures."
    }
  ],
  // This is the key currently used for explicit prompt guidance for tags:
  "instructions": "Focus on identifying professional actions in an office setting."
}
```

## Output Structure

The analysis creates a structured output directory:

*   `_ActionSummary.json`: (Or named based on the analysis config, e.g., `_BasicSummary.json`). This is the **final, aggregated output file**. It contains processed chapters, global tags (persons, actions, objects) with merged time intervals, metadata, and copyright info (if provided).
    *   **Persons tags** benefit from the persistent IDs generated by the YOLO tracking and refinement pipeline, leading to consistent identification and de-duplication. Each person entry will have an `id` (from YOLO tracker), a `name` (often an LLM-generated description), `yoloClass` ("person"), and a list of `timecodes`.
    *   If transcription was performed, it also includes a `transcript` field with the full concatenated text and a `transcriptionDetails` object with structured segments.
    This file is generated by the `process_segment_results` method of the chosen analysis configuration (e.g., `ActionSummary`).
*   `_video_manifest.json`: Contains all processing metadata, source video info, segment definitions.
    *   It stores the `raw_yolo_tags` (direct output from `step0_yolo_track.py`).
    *   If track refinement is run, it also stores `refined_yolo_tags` (output from `track_refiner.py`). These include LLM-generated descriptions and more stable IDs for tracks.
    *   It also contains the *raw, unaggregated* analysis results returned by the LLM for each segment/chunk *before* the final aggregation and merging step.
    *   If Azure Batch Transcription is used, the full raw JSON output from the Azure service is typically stored here.
    *   **Note:** After final aggregation into `_ActionSummary.json`, the `global_tags` within individual segment results in *this* manifest file might be removed for clarity or represent an earlier stage of processing.
*   `{video_name}_manifest_raw_yolo_tags.json`: A separate file containing just the raw YOLO tracking data from `step0_yolo_track.py`.
*   `{video_name}_manifest_refined_yolo_tags.json`: If refinement is run, this file contains just the refined YOLO tracking data from `track_refiner.py`.
*   Segment Folders (`seg_...` or `scene_...`): Each segment has its own folder containing:
    *   `frames/`: Extracted image frames (named with their absolute timestamps, e.g., `frame_0_12.345s.jpg`). These are used for LLM analysis.
    *   `_segment_analyzed_result_ActionSummary.json`: The raw JSON output from the LLM for that segment (including both chapter and chunked tag results before final aggregation).
    *   `_segment_prompt.json`: The prompt sent to the LLM for this segment/chunk.
*   `yolo_thumbs_raw/`: Directory (within the main output directory) containing cropped thumbnail images for each raw YOLO track, generated by `step0_yolo_track.py`.
*   `yolo_refined_thumbs/`: Directory (within the main output directory) containing cropped thumbnail images for each refined YOLO track, generated by `track_refiner.py`.

## How it Works: Overview of Analysis Pipeline

The tool employs a multi-stage pipeline:

1.  **Preprocessing (`VideoPreProcessor`):**
    *   Extracts video metadata (duration, FPS, etc.).
    *   Performs segmentation based on scenes, speech pauses, or fixed time.
    *   Extracts frames at the specified `--fps` for each segment (these are for LLM analysis).
    *   Optionally, performs audio transcription using Azure Batch Transcription.
    *   Extracts dominant colors from frames.
2.  **Raw Object/Person Tracking (`step0_yolo_track.py`):**
    *   The entire video is processed by a YOLO model (e.g., YOLOv8, YOLO-NAS) with an object tracker (e.g., ByteTrack).
    *   This step identifies objects and persons, assigning them initial track IDs and recording their bounding boxes for each frame they appear in.
    *   Representative thumbnails are generated for each raw track.
    *   The output is a list of `raw_yolo_tags` stored in the manifest and a separate JSON file.
3.  **Track Refinement (`track_refiner.py` - Optional):**
    *   Takes the `raw_yolo_tags` and `segments` from the manifest.
    *   For tracks (especially persons) that span across multiple detected scene segments, an LLM (GPT-4 Vision) is used to:
        *   Visually compare appearances of track segments across scenes.
        *   Merge or link tracks that belong to the same person/object, assigning a more persistent `refined_track_id`.
        *   Generate a concise descriptive label (e.g., "Woman in red dress") for each refined track based on its visual features.
    *   Thumbnails are also generated for these refined tracks.
    *   The output is a list of `refined_yolo_tags` stored in the manifest and a separate JSON file. If this step is skipped (via `--skip-refinement`), the `raw_yolo_tags` are used downstream.
4.  **Main LLM Analysis (`VideoAnalyzer`):**
    *   For each segment:
        *   **Chapter Generation:** To understand the narrative and overall content, the LLM analyzes *all* frames (extracted by `VideoPreProcessor`) and the full transcription (if available) for that segment. The active YOLO tracks (refined or raw) within the segment's timeframe are also provided as context.
        *   **Tag Generation (Persons, Actions, Objects):**
            *   The segment's frames are processed by the LLM.
            *   The (refined or raw) YOLO tracks active in the segment, along with their IDs and descriptions (if available from refinement), are crucial context.
            *   For **persons**, the LLM's output is later reconciled with the YOLO track IDs to ensure consistency.
            *   For **objects**, YOLO detections inform the LLM. `VideoAnalyzer` might also use an LLM to generate more specific descriptions for YOLO-detected objects if they weren't fully described during refinement, using cropped images.
            *   **Actions** are identified by the LLM based on visual and transcript information, often in relation to the identified persons and objects. A dedicated action extraction LLM call might also be made per segment to get more granular action details.
5.  **Aggregation and Final Output (`ActionSummary.process_segment_results`):**
    *   The results from all segments (chapters and tags) are collected.
    *   **Global Tags** (persons, actions, objects) are aggregated:
        *   For **persons**, entries are grouped by their `id` (from YOLO/refinement). Timecodes are merged, and a single representative `name` (description) is chosen (usually the first one encountered for that ID or the one from refinement).
        *   For **objects** and **actions**, entries are grouped by their `name`. Timecodes are merged.
    *   A final summary and description for the entire video are generated.
    *   The final structured data is saved to `_ActionSummary.json`.

This multi-layered approach, combining traditional computer vision (YOLO) with advanced LLM capabilities, aims to provide both high-level understanding (chapters, summary) and detailed, temporally accurate tagging of entities and activities.

## Note on Frame Storage
In production environments, consider strategies for managing extracted frames (e.g., storing them externally, deleting after processing) as they can consume significant disk space. Currently, all extracted frames for LLM analysis are saved and not deleted when the process finishes. YOLO track thumbnails are also saved.

## Operational Notes

*   **Azure Blob Storage Cleanup:** When using Azure Batch Transcription, you might observe `BlobNotFound` errors in the logs during the cleanup phase for temporary audio files uploaded to Azure Blob Storage. This is often normal. The Azure Speech service itself may delete the input audio blob from the container after it has successfully processed the transcription. The tool attempts to delete these blobs as a precautionary cleanup measure, so these errors usually indicate the file was already cleaned up by Azure.
*   **YOLO Model Files:** Ensure the YOLO model weights (e.g., `.pt` files) and tracker configuration (e.g., `.yaml` files) are available in the expected location (typically `src/cobrapy/pipeline/`) for the `step0_yolo_track.py` script to function correctly.
