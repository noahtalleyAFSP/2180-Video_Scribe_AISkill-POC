# Cobra Video Analysis Tool

This tool analyzes videos using Large Language Models (like GPT-4 Vision) to provide structured descriptions of content, including chapters, persons, actions, and objects, with a focus on generating accurate timestamps and leveraging advanced object tracking.

## Features

-   **Flexible Segmentation:**
    *   Segments video based on content using **PySceneDetect** (if `--use-scene-detection` is enabled, highest priority).
    *   Falls back to segmentation based on **natural speech pauses** found in the transcription (if `--use-speech-based-segments` is enabled, audio is present, and scene detection is disabled).
    *   Uses **fixed-time intervals** (`--segment-duration`) as the final fallback method.
-   **Advanced Object/Person Tracking with YOLO:**
    *   **Initial Raw Detection:** Employs YOLO (e.g., `yolo11x.pt` with ByteTrack via `step0_yolo_track.py`) for initial detection and tracking of objects and persons, generating raw track data with frame-level bounding boxes and preliminary IDs.
    *   **Two-Step Track Refinement (NEW):** An optional but highly recommended two-step process (`--skip-refinement` to disable) uses an LLM to achieve robust cross-scene person re-identification.
        *   **1. Splitting at Scene Cuts (`step0b_reid_split_tracks.py`):** First, any raw person tracks that span across scene cuts are automatically split. This prevents incorrect merges and prepares the tracks for more accurate comparison.
        *   **2. Cross-Scene Re-Identification (`track_refiner.py`):** The split track segments are then compared across scene boundaries. An LLM visually compares a person's appearance just before a cut to their appearance just after, intelligently linking them and assigning a stable, persistent `refined_track_id`. It also generates descriptive labels (e.g., "Man in blue shirt") for these refined tracks.
    *   **Contextual Enhancement:** The final refined (or raw, if refinement is skipped) YOLO tracks, along with their generated descriptions and IDs, are provided as context to the main LLM analysis for each segment. This helps the LLM maintain awareness of known entities throughout the video.
-   **Dual Analysis Strategy:**
    *   **Chapters:** Generated by analyzing the full context (frames, transcription, relevant YOLO tracks) of each video segment for narrative understanding.
    *   **Tags (Persons, Actions, Objects):**
        *   **Persons:** Leverages the (refined) YOLO track IDs for consistent identification. Descriptions are often derived from the track refinement step or further LLM analysis of track appearances. The final aggregation in `ActionSummary` groups persons by their stable `id`, merges their timecodes, and selects a representative `classDescription`.
        *   **Objects:** Similar to persons, object tracks from YOLO are used. If not fully described during refinement, `VideoAnalyzer` might use an LLM to generate a descriptive label for a detected object based on its visual crop. These are then aggregated by `classDescription`.
        *   **Actions:** Identified through LLM analysis of segment frames and transcription, potentially informed by the presence of tracked persons/objects. A dedicated fine-grained action extraction step can also run per segment. These are then aggregated by `classDescription`.
-   **Timestamp Accuracy:** Aims for second-level accuracy in global tags by providing the LLM with frames timestamped precisely during preprocessing (configurable via `--fps`). YOLO tracking also provides frame-level timestamps.
    *   The `ActionSummary` configuration includes logic for normalizing tag `classDescription`s and merging adjacent or overlapping time intervals for the same tag (or same person ID) based on a configurable time gap during the final aggregation step.
-   **Frame Handling:** Currently processes all extracted frames within each segment/chunk for LLM analysis. YOLO processes the video at its native frame rate.
-   **Transcription Integration:** Leverages **Azure Batch Transcription** (if audio is available and configured). This provides:
    *   A full transcript string within the `_ActionSummary.json`.
    *   Detailed `transcriptionDetails` in `_ActionSummary.json`, including per-segment information (speaker, timings, text variants).
    *   Richer context for both chapter and tag generation.
    *   Requires Azure Speech and Blob Storage setup.
-   **Dominant Color Extraction:** Extracts dominant colors from video frames during preprocessing, providing these as hex values for scene/segment characterization in the output.
-   **Customizable Entity Recognition:** Allows providing lists of known persons, actions, objects, emotions, and themes to guide the analysis. The primary mechanism for this is the "instructions" field in these JSON lists.
-   **Tag Post-Processing:** The `ActionSummary` configuration includes logic for normalizing tag names and merging adjacent or overlapping time intervals for the same tag (or same person ID) based on a configurable time gap during the final aggregation step.
-   **Asynchronous Processing:** Supports parallel processing of segments for faster analysis on multi-core systems (`--run-async-analyzer`). The track refinement step also utilizes asynchronous LLM calls.
-   **Structured JSON Output:** Produces detailed JSON files containing the analysis results, including a final aggregated summary file.

## Installation

1.  Ensure you have Python 3.8+ and `ffmpeg` installed and available in your system's PATH.
2.  Clone the repository.
3.  Install the required Python packages:
    ```bash
    pip install -r requirements.txt 
    ```
    This includes `ultralytics` for YOLO, `opencv-python` for video processing, and `scenedetect` among others.
4.  **YOLO Models:** The `step0_yolo_track.py` script expects YOLO model files (e.g., `yolo11x.pt` for object detection) and tracker configuration files (e.g., `bytetrack.yaml`). These files should typically be placed in the `src/cobrapy/pipeline/` directory or be accessible from there. If they are not found, the YOLO tracking step will fail. *(Note: Specific model versions and paths might be hardcoded or configurable via script arguments in the future; check `run_yolo` function in `step0_yolo_track.py` for current expectations).*
5.  Set up your API credentials (e.g., Azure OpenAI, Azure Speech, Azure Storage) either via environment variables or a `.env` file (see Configuration).

## Configuration

API credentials and endpoints are required. You can configure them using:

1.  **Environment Variables:**
    *   `AZURE_OPENAI_GPT_VISION_ENDPOINT` (Required for analysis)
    *   `AZURE_OPENAI_GPT_VISION_API_KEY` (Required for analysis)
    *   `AZURE_OPENAI_GPT_VISION_API_VERSION` (Required for analysis)
    *   `AZURE_OPENAI_GPT_VISION_DEPLOYMENT` (Required for analysis)
    *   `AZURE_SPEECH_KEY` (Required for transcription)
    *   `AZURE_SPEECH_REGION` (Required for transcription)
    *   `AZURE_STORAGE_ACCOUNT_NAME` (Required for transcription - audio upload)
    *   `AZURE_STORAGE_CONTAINER_NAME` (Required for transcription - audio upload)
    *   `AZURE_STORAGE_CONNECTION_STRING` (Required for transcription - OR use SAS_TOKEN)
    *   `AZURE_STORAGE_SAS_TOKEN` (Required for transcription - OR use CONNECTION_STRING)
    *   (Optional) `AZURE_FACE_ENDPOINT`, `AZURE_FACE_API_KEY` (for face recognition - *Currently experimental*)
2.  **`.env` File:** Create a `.env` file in the project root or specify a path using `--env-file`.
3.  **Command Line Arguments:** Override environment settings using arguments like `--api-key`, `--api-base`, etc.

## Usage

Analysis using scene detection (recommended for content-aware segments), including YOLO refinement:

```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --use-scene-detection --scene-detection-threshold 27.0
```

To skip the YOLO track refinement step (e.g., for faster processing or if raw tracks are sufficient):
```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --use-scene-detection --skip-refinement
```

Analysis using time-based segments:

```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --segment-duration 60
```

Analysis using speech-based segmentation (requires audio and transcription configuration):

```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --use-speech-based-segments
```

Analysis with custom lists and asynchronous segment processing:

```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --fps 1.0 --run-async-analyzer --actions-list path/to/actions.json --objects-list path/to/objects.json --use-scene-detection
```

### Command Line Arguments

*   `video_path`: Path to the video file (required).
*   `--output-dir`: Directory to save analysis results (default: `./{video_name}_analysis`).
*   `--env-file`: Path to a custom `.env` file.
*   `--api-key`: Vision API key (overrides environment).
*   `--api-base`: Vision API endpoint (overrides environment).
*   `--api-version`: Vision API version (overrides environment).
*   `--deployment-name`: Vision API deployment name (overrides environment).
*   `--fps`: Frames per second to extract during preprocessing for LLM analysis (default: 1.0). Affects granularity available for LLM-based tagging. YOLO may process at the video's native FPS.
*   `--use-scene-detection`: Use PySceneDetect to determine segments instead of fixed intervals or speech pauses (Highest priority).
*   `--scene-detection-threshold`: PySceneDetect `ContentDetector` threshold (default: 30.0). Lower value generally means more, shorter scenes. Used only if `--use-scene-detection` is active.
*   `--use-speech-based-segments`: Use speech analysis to define segment boundaries. Ignored if `--use-scene-detection` is active. Requires audio and successful transcription.
*   `--segment-duration`: Default length (seconds) for time-based video segments (default: 30.0). Used only if scene detection and speech-based segmentation are inactive or fail.
*   `--transcription-path`: Path to an existing JSON transcription file (if not generating via Azure).
*   `--run-async-analyzer`: Run VideoAnalyzer's segment processing asynchronously (default: False).
*   `--overwrite-output`: **(NEW)** If set, will overwrite the output directory if it exists. Use with caution.
*   `--enable-language-id`: **(NEW)** Enable Azure Batch Transcription's language identification feature.
*   `--skip-refinement`: Skip the YOLO track refinement steps (both splitting and Re-ID) (default: False). If skipped, raw YOLO tracks are used directly.
*   `--skip-thumbnail-saving`: If set, skips saving of YOLO and Re-ID thumbnails (default: False, thumbnails are saved).
*   `--peoples-list`: Path to JSON file defining known people.
*   `--emotions-list`: Path to JSON file defining relevant emotions.
*   `--objects-list`: Path to JSON file defining known objects.
*   `--themes-list`: Path to JSON file defining potential themes.
*   `--actions-list`: Path to JSON file defining known actions.
*   `--copyright-file`: Path to a JSON file containing copyright information to be included in the `ActionSummary` output.
*   `--downscale-to-max-width`: Maximum width for downscaled frames (maintains aspect ratio).
*   `--downscale-to-max-height`: Maximum height for downscaled frames (maintains aspect ratio).

### Custom List Format

Custom lists (e.g., for persons, objects, actions) allow providing domain-specific guidance to the LLM. The tool expects a JSON file containing:
1.  A top-level key matching the list type (e.g., `"actions"`, `"persons"`, `"objects"`). The value should be an array of objects.
2.  *(Optional but Recommended)* A top-level key named `"instructions"` containing a string of text. **This `instructions` text is the primary mechanism currently used to customize the LLM's behavior for tag generation.** It is directly injected into the system prompt for identifying persons, actions, and objects.
3.  Each object within the array (e.g., under `"actions"`) typically contains:
    *   A key for the item's name (e.g., `"name"` or `"label"` - the exact key name used here is **not** currently critical for prompt generation, although `"classDescription"` now aligns with the final output structure).
    *   A `"description"` key.

**Important:** While the structure including item names (`name`/`label`/`classDescription`) and `description` is loaded, the current implementation for tag generation **only explicitly uses the top-level `"instructions"` string** in the prompt. The individual item `classDescription` and `description` fields are **not** currently formatted into the tag generation prompts. They serve primarily as user reference or may provide *implicit* context if the `classDescription` values are common terms the LLM recognizes.

Example (`actions.json`):

```json
{
  "actions": [ // Top-level key matching list type
    {
      "classDescription": "Typing", // Item name (key doesn't strictly matter for prompt, 'classDescription' matches output)
      "description": "Person using a keyboard." // Description (currently not used in prompt)
    },
    {
      "classDescription": "Presenting",
      "description": "Person speaking to an audience, possibly using slides or gestures."
    }
  ],
  // This is the key currently used for explicit prompt guidance for tags:
  "instructions": "Focus on identifying professional actions in an office setting."
}
```

## Output Structure

The analysis creates a structured output directory:

*   `_ActionSummary.json`: This is the **final, aggregated output file**. It contains processed chapters and global tags.
    *   **Tags (persons, actions, objects):** Aggregated from all segments with merged time intervals.
        *   **Persons tags** benefit from the persistent IDs generated by the YOLO tracking and refinement pipeline. Each person entry will have an `id` (the stable integer ID from the tracker), a `classDescription` (an LLM-generated description like "woman in red dress"), `yoloClass` ("person"), and a list of `timecodes`.
    *   **`processing_info` (NEW):** This new object inside `actionSummary` contains valuable run metadata, including `runtime_seconds` and a detailed `tokens_used` breakdown for different stages of the analysis (Re-ID, chapter generation, tag description, etc.).
    *   **`transcript`**: If transcription was performed, this field holds the full concatenated text.
*   `_video_manifest.json`: Contains all processing metadata, source video info, and segment definitions.
    *   It stores the `raw_yolo_tags` (direct output from `step0_yolo_track.py`) and, if refinement is run, the final `refined_yolo_tags` (output from `track_refiner.py`).
    *   It also contains the *raw, unaggregated* analysis results returned by the LLM for each segment *before* the final aggregation.
*   **Intermediary Files (NEW - DELETED AFTER RUN):** During the run, several JSON files are created for the pipeline steps (e.g., `..._raw_yolo_tags.json`, `..._cuts.json`, `..._reid_split_refined_yolo_tags.json`). To save space, these are **automatically deleted** at the end of a successful analysis.
*   Segment Folders (`seg_...` or `scene_...`): Each segment has its own folder containing extracted frames, the raw LLM prompt, and the raw LLM result for that segment.
*   **Thumbnail Directories (NEW STRUCTURE):**
    *   `yolo_thumbs_raw/`: Contains cropped thumbnails for each raw YOLO track, generated by `step0_yolo_track.py`.
    *   `yolo_thumbs_reid_split/`: **(NEW)** Contains thumbnails for tracks that were split at scene cuts, generated by `step0b_reid_split_tracks.py`.
    *   `refined_track_thumbs/`: Contains thumbnails for the final, merged/refined tracks, generated by `track_refiner.py`.

## How it Works: Overview of Analysis Pipeline

The tool employs a multi-stage pipeline:

1.  **Preprocessing (`VideoPreProcessor`):**
    *   Extracts video metadata and performs segmentation (scenes, speech, or time-based).
    *   Extracts frames at the specified `--fps` for LLM analysis.
    *   Optionally, performs audio transcription via Azure Batch Transcription.
2.  **Raw Object/Person Tracking (`step0_yolo_track.py`):**
    *   The entire video is processed by a YOLO model with an object tracker (e.g., ByteTrack).
    *   This step identifies objects and persons, assigning them initial track IDs and recording their bounding boxes and a representative thumbnail for each raw track (saved to `yolo_thumbs_raw/`). The key for the thumbnail path in the JSON output is `thumb`.
    *   The output is a list of `raw_yolo_tags`.
3.  **Track Refinement (Two-Step Process - Optional):** This is skipped if `--skip-refinement` is used.
    *   **3a. Track Splitting at Scene Cuts (`step0b_reid_split_tracks.py`):**
        *   Takes the `raw_yolo_tags` and scene cut timestamps.
        *   Any person track that spans across a scene cut is split into separate track segments.
        *   This crucial step prevents the LLM from trying to compare a person who has completely changed appearance/outfit between scenes. Thumbnails for these split tracks are saved to `yolo_thumbs_reid_split/`.
    *   **3b. Cross-Scene Re-Identification (`track_refiner.py`):**
        *   Takes the track segments from the previous step.
        *   For each scene cut, an LLM visually compares a person's appearance *before* the cut to their appearance *after*.
        *   Based on the LLM's decision, it merges the track segments, assigning a persistent `refined_track_id`. It also generates a concise descriptive label (e.g., "Woman in red dress") for the final refined track.
        *   Thumbnails for these final tracks are saved to `refined_track_thumbs/`.
4.  **Main LLM Analysis (`VideoAnalyzer`):**
    *   For each video segment, the analyzer uses the frames, transcription, and YOLO track data (preferring refined tags if available) as context.
    *   It performs multiple LLM calls to generate chapters, identify actions, and describe persons and objects with high fidelity.
5.  **Aggregation and Final Output (`ActionSummary.process_segment_results`):**
    *   The results from all segments are collected.
    *   **Global Tags** (persons, actions, objects) are aggregated. For persons, entries are grouped by their stable `id` from the tracker. For objects/actions, they are grouped by `classDescription`. Timecodes are merged for all.
    *   A final summary and description for the entire video are generated.
    *   The final structured data is saved to `_ActionSummary.json`.
6.  **Cleanup (NEW):**
    *   At the end of the run, intermediary JSON files created during the pipeline (raw tags, refined tags, cuts) are deleted to keep the output directory clean.

This multi-layered approach, combining traditional computer vision (YOLO) with a new, robust two-stage LLM-based re-identification pipeline, provides both high-level understanding and detailed, temporally accurate tagging.

## Note on Frame Storage
In production environments, consider strategies for managing extracted frames (e.g., storing them externally, deleting after processing) as they can consume significant disk space. Currently, all extracted frames for LLM analysis are saved and not deleted when the process finishes. YOLO track thumbnails are also saved.

## Operational Notes

*   **Azure Blob Storage Cleanup:** When using Azure Batch Transcription, you might observe `BlobNotFound` errors in the logs during the cleanup phase for temporary audio files uploaded to Azure Blob Storage. This is often normal. The Azure Speech service itself may delete the input audio blob from the container after it has successfully processed the transcription. The tool attempts to delete these blobs as a precautionary cleanup measure, so these errors usually indicate the file was already cleaned up by Azure.
*   **YOLO Model Files:** Ensure the YOLO model weights (e.g., `.pt` files) and tracker configuration (e.g., `.yaml` files) are available in the expected location (typically `src/cobrapy/pipeline/`) for the `step0_yolo_track.py` script to function correctly.