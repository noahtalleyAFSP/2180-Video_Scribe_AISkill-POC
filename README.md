# Cobra Video Analysis Tool

This tool analyzes videos using Large Language Models (like GPT-4 Vision) to provide structured descriptions of content, including chapters, persons, actions, and objects, with a focus on generating accurate timestamps.

## Features

-   **Dual Analysis Strategy:**
    -   **Chapters:** Generated by analyzing the full context (frames, transcription) of each video segment for narrative understanding.
    -   **Tags (Persons, Actions, Objects):** Generated by analyzing segments in smaller, consecutive time chunks (e.g., 5 seconds) using frames and their precise timestamps to achieve higher temporal accuracy for when entities appear or actions occur.
-   **Timestamp Accuracy:** Aims for second-level accuracy in global tags by providing the LLM with frames timestamped precisely during preprocessing (typically 1 frame per second, configurable via `--fps`).
-   **Frame Handling:** Currently processes all extracted frames within each segment/chunk. Future optimizations may involve intelligent frame selection (e.g., keyframes), but the current method maximizes the information available to the LLM for tagging.
-   **Transcription Integration:** Leverages audio transcriptions (if available) for richer context in both chapter and tag generation.
-   **Speech-Based Segmentation:** Optionally segments the video based on natural speech pauses found in the transcription, potentially creating more contextually relevant segments than fixed-time intervals.
-   **Customizable Entity Recognition:** Allows providing lists of known persons, actions, objects, emotions, and themes to guide the analysis.
-   **Tag Post-Processing:** Includes logic for normalizing tag names (e.g., "Walking" vs "walking") and merging adjacent or overlapping time intervals for the same tag based on a configurable time gap.
-   **Asynchronous Processing:** Supports parallel processing of segments and tagging chunks for faster analysis on multi-core systems.
-   **Structured JSON Output:** Produces detailed JSON files containing the analysis results.

## Installation

1.  Ensure you have Python 3.8+ and `ffmpeg` installed and available in your system's PATH.
2.  Clone the repository.
3.  Install the required Python packages:

```bash
pip install -r requirements.txt
```

4.  Set up your API credentials (e.g., Azure OpenAI) either via environment variables or a `.env` file (see Configuration).

## Configuration

API credentials and endpoints are required. You can configure them using:

1.  **Environment Variables:**
    *   `AZURE_OPENAI_GPT_VISION_ENDPOINT`
    *   `AZURE_OPENAI_GPT_VISION_API_KEY`
    *   `AZURE_OPENAI_GPT_VISION_API_VERSION`
    *   `AZURE_OPENAI_GPT_VISION_DEPLOYMENT`
    *   (Optional) `AZURE_SPEECH_KEY`, `AZURE_SPEECH_REGION` (for transcription)
    *   (Optional) `AZURE_FACE_ENDPOINT`, `AZURE_FACE_API_KEY` (for face recognition - *Currently experimental*)
2.  **`.env` File:** Create a `.env` file in the project root or specify a path using `--env-file`.
3.  **Command Line Arguments:** Override environment settings using arguments like `--api-key`, `--api-base`, etc.

## Usage

Basic analysis using time-based segments:

```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output
```

Analysis using speech-based segmentation (requires audio and transcription):

```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --use-speech-based-segments
```

Analysis with custom lists and asynchronous processing:

```bash
python analyze_video.py my_video.mp4 --output-dir ./my_video_output --fps 1.0 --async --actions-list path/to/actions.json --objects-list path/to/objects.json
```

### Command Line Arguments

*   `video_path`: Path to the video file (required).
*   `--output-dir`: Directory to save analysis results (default: `./{video_name}_analysis`).
*   `--env-file`: Path to a custom `.env` file.
*   `--api-key`: Vision API key (overrides environment).
*   `--api-base`: Vision API endpoint (overrides environment).
*   `--api-version`: Vision API version (overrides environment).
*   `--deployment-name`: Vision API deployment name (overrides environment).
*   `--fps`: Frames per second to extract during preprocessing (default: 1.0). Affects granularity available for tagging.
*   `--segment-duration`: Default length (seconds) for time-based video segments (default: 30.0). Ignored if `--use-speech-based-segments` is active.
*   `--transcription-path`: Path to an existing JSON transcription file (if not generating).
*   `--use-speech-based-segments`: Use speech analysis to define segment boundaries instead of fixed `segment-duration`. Requires audio.
*   `--async`: Run segment/chunk analysis asynchronously using multiple processes/threads (default: False).
*   `--peoples-list`: Path to JSON file defining known people.
*   `--emotions-list`: Path to JSON file defining relevant emotions.
*   `--objects-list`: Path to JSON file defining known objects.
*   `--themes-list`: Path to JSON file defining potential themes.
*   `--actions-list`: Path to JSON file defining known actions.

### Custom List Format

Custom lists help guide the LLM. Example (`actions.json`):

```json
{
  "actions": [
    {
      "name": "Typing",
      "description": "Person using a keyboard."
    },
    {
      "name": "Presenting",
      "description": "Person speaking to an audience, possibly using slides or gestures."
    }
  ],
  "instructions": "Focus on identifying professional actions in an office setting." // Optional instructions
}

```

## Output Structure

The analysis creates a structured output directory:

*   `_video_manifest.json`: The main file containing all metadata, segment information, aggregated chapters, and final aggregated/merged global tags (persons, actions, objects). **Note:** After final aggregation, the `global_tags` within individual segment results are removed from this file for clarity.
*   `_ActionSummary.json`: (Or based on analysis name) Contains the final aggregated chapters and global tags, potentially including a final summary text if enabled.
*   Segment Folders (`seg_...`): Each segment has its own folder containing:
    *   `frames/`: Extracted image frames (named with their absolute timestamps).
    *   `_segment_analyzed_result_ActionSummary.json`: The raw JSON output from the LLM for that segment (including both chapter and chunked tag results before final aggregation).
    *   `_segment_prompt.json`: (Potentially) The prompt sent to the LLM for this segment/chunk.

## How it Works: Chapters vs. Tags

The tool employs two distinct strategies for analysis:

1.  **Chapter Generation:** To understand the narrative and overall content of a segment, the LLM analyzes *all* frames and the full transcription (if available) for that segment in a single call. This provides broad context for generating a descriptive chapter summary, theme, etc.
2.  **Tag Generation (Persons, Actions, Objects):** To achieve accurate start and end times for specific tags, the analysis is done differently. Each segment is broken down into smaller, consecutive time *chunks* (e.g., 5 seconds). The LLM analyzes the frames *within each chunk*, using their precise timestamps. This forces the LLM to focus on a shorter time window, significantly improving the accuracy of the `start` and `end` timecodes associated with each detected person, action, or object. The results from all chunks are then merged and post-processed (normalized, time intervals combined) to create the final `global_tags`.

This dual approach balances the need for broad segment understanding (for chapters) with the need for temporal precision (for tags).

## Note: In production the frames/thumbnails will be saved so they are not generated later on. Currently all frames are saved and not deleted when a process finishes.